{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brOUkAOg9JD0"
      },
      "source": [
        "# **Identification**\n",
        "\n",
        "* **Name:** Joaquim Daniel Rios da Cunha\n",
        "* **Student Number:** 201806651\n",
        "\n",
        "* **Name:** Pedro Miguel Pinto Silva\n",
        "* **Student Number:** 201806526"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7akJFXH99Lxw"
      },
      "source": [
        "# Context\n",
        "\n",
        "The problem was divided in two steps (cascade process), corresponding to 2 different datasets:\n",
        "- Detection of ROI, that is, the region of the image that have the license plate\n",
        "- Character extraction, that is, identify the region of each character individually and get the corresponding character until having the full license plate \n",
        "\n",
        "Each dataset has its own labels.\n",
        "- https://www.kaggle.com/datasets/andrewmvd/car-plate-detection\n",
        "- https://www.kaggle.com/datasets/francescopettini/license-plate-characters-detection-ocr\n",
        "\n",
        "We approached this problem based on the paper \"Computer Vision based License Plate Detection for Automated Vehicle Parking Management System\", https://ieeexplore.ieee.org/document/9298091\n",
        "\n",
        "In Part A and in the first method of Part B we followed the methods explained in this paper. As the results were not what was expected in Part B, we tried our own method based on OpenCV operations that we learned in this course. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0DRWOYdnIiR"
      },
      "source": [
        "# Part A - License Plate Detection using YOLOv5 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8657OkkbOwBz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import yaml\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import xml.etree.ElementTree as xet\n",
        "\n",
        "\n",
        "from glob import glob\n",
        "from skimage import io\n",
        "from shutil import copy\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLBCk1UinV2a"
      },
      "source": [
        "## 1\\. Load dataset for YOLO v5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vhwn2byy9XI"
      },
      "outputs": [],
      "source": [
        "train = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpDGz1zfCDZf",
        "outputId": "36012276-855f-4fec-9ae7-1b32169ce051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-09 17:41:00--  https://uporto-my.sharepoint.com/:u:/g/personal/up201806651_up_pt/ETX4xFICL9ZNl15HJkxlMgQBaGpdysmeNvWkC26zh-6Ccg?download=1\n",
            "Resolving uporto-my.sharepoint.com (uporto-my.sharepoint.com)... 13.107.136.8, 13.107.138.8, 2620:1ec:8f8::8, ...\n",
            "Connecting to uporto-my.sharepoint.com (uporto-my.sharepoint.com)|13.107.136.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /personal/up201806651_up_pt/Documents/5ano/1S/CV/projects/Assig2/dataset1.zip?ga=1 [following]\n",
            "--2022-12-09 17:41:01--  https://uporto-my.sharepoint.com/personal/up201806651_up_pt/Documents/5ano/1S/CV/projects/Assig2/dataset1.zip?ga=1\n",
            "Reusing existing connection to uporto-my.sharepoint.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 212891379 (203M) [application/x-zip-compressed]\n",
            "Saving to: ‘dataset1.zip’\n",
            "\n",
            "dataset1.zip        100%[===================>] 203.03M  51.6MB/s    in 5.6s    \n",
            "\n",
            "2022-12-09 17:41:07 (36.5 MB/s) - ‘dataset1.zip’ saved [212891379/212891379]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# downloading datasets\n",
        "if not os.path.exists('dataset1'):\n",
        "    !wget --output-document=dataset1.zip \"https://uporto-my.sharepoint.com/:u:/g/personal/up201806651_up_pt/ETX4xFICL9ZNl15HJkxlMgQBaGpdysmeNvWkC26zh-6Ccg?download=1\"\n",
        "    !unzip -q dataset1.zip\n",
        "    !rm dataset1.zip\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwncAqwBrZyw"
      },
      "source": [
        "## 2\\. Dataset information extraction (xml readings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = glob('./dataset1/annotations/*.xml')\n",
        "labels_dict = dict(filepath=[],xmin=[],xmax=[],ymin=[],ymax=[])\n",
        "for filename in path:\n",
        "\n",
        "    info = xet.parse(filename)\n",
        "    root = info.getroot()\n",
        "    member_object = root.find('object')\n",
        "    labels_info = member_object.find('bndbox')\n",
        "    xmin = int(labels_info.find('xmin').text)\n",
        "    xmax = int(labels_info.find('xmax').text)\n",
        "    ymin = int(labels_info.find('ymin').text)\n",
        "    ymax = int(labels_info.find('ymax').text)\n",
        "\n",
        "    labels_dict['filepath'].append(filename)\n",
        "    labels_dict['xmin'].append(xmin)\n",
        "    labels_dict['xmax'].append(xmax)\n",
        "    labels_dict['ymin'].append(ymin)\n",
        "    labels_dict['ymax'].append(ymax)\n",
        "\n",
        "df = pd.DataFrame(labels_dict)\n",
        "df.to_csv('labels.csv',index=False)\n",
        "df.head()\n",
        "\n",
        "filename = df['filepath'][0]\n",
        "def getFilename(filename):\n",
        "    filename_image = xet.parse(filename).getroot().find('filename').text\n",
        "    filepath_image = os.path.join('./dataset1/images',filename_image)\n",
        "    return filepath_image\n",
        "getFilename(filename)\n",
        "\n",
        "image_path = list(df['filepath'].apply(getFilename))\n",
        "image_path[:10]   #random check\n",
        "\n",
        "file_path = image_path[87]  \n",
        "img = cv2.imread(file_path) \n",
        "\n",
        "cv2_imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuqH1vfgCiAN"
      },
      "source": [
        "## 3\\. Load YOLOv5 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C8sC4NgKTzP"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "!pip install -r ./yolov5/requirements.txt\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgaMmxuLMxe2"
      },
      "outputs": [],
      "source": [
        "!mkdir ./yolov5/data_images/\n",
        "!mkdir ./yolov5/data_images/test/\n",
        "!mkdir ./yolov5/data_images/train/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnZAejwuNmNL"
      },
      "source": [
        "### 3.1 Modifying the dataset to be compatible with YOLO\n",
        "The original dataset has the coordinates for the bounding box of each license plate.\n",
        "\n",
        "The YOLO operates with the center coordinates and the width and height, so it is necessary to adjust.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XKiisfXsJ1E"
      },
      "outputs": [],
      "source": [
        "# parsing\n",
        "def parsing(path):\n",
        "    parser = xet.parse(path).getroot()\n",
        "    name = parser.find('filename').text\n",
        "    filename = f'./dataset1/images/{name}'\n",
        "\n",
        "    # width and height\n",
        "    parser_size = parser.find('size')\n",
        "    width = int(parser_size.find('width').text)\n",
        "    height = int(parser_size.find('height').text)\n",
        "    \n",
        "    return filename, width, height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vwVuU8nRKj34",
        "outputId": "b1f6d7be-3de0-4d85-f925-c3bd45aca806"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-69567531-7c93-4dde-a97d-505699ee7dd2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filepath</th>\n",
              "      <th>xmin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymin</th>\n",
              "      <th>ymax</th>\n",
              "      <th>filename</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>center_x</th>\n",
              "      <th>center_y</th>\n",
              "      <th>bb_width</th>\n",
              "      <th>bb_height</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>./dataset1/annotations/Cars262.xml</td>\n",
              "      <td>243</td>\n",
              "      <td>285</td>\n",
              "      <td>184</td>\n",
              "      <td>205</td>\n",
              "      <td>./dataset1/images/Cars262.png</td>\n",
              "      <td>400</td>\n",
              "      <td>301</td>\n",
              "      <td>0.66000</td>\n",
              "      <td>0.646179</td>\n",
              "      <td>0.1050</td>\n",
              "      <td>0.069767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>./dataset1/annotations/Cars285.xml</td>\n",
              "      <td>91</td>\n",
              "      <td>163</td>\n",
              "      <td>111</td>\n",
              "      <td>134</td>\n",
              "      <td>./dataset1/images/Cars285.png</td>\n",
              "      <td>400</td>\n",
              "      <td>267</td>\n",
              "      <td>0.31750</td>\n",
              "      <td>0.458801</td>\n",
              "      <td>0.1800</td>\n",
              "      <td>0.086142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>./dataset1/annotations/Cars131.xml</td>\n",
              "      <td>19</td>\n",
              "      <td>497</td>\n",
              "      <td>29</td>\n",
              "      <td>245</td>\n",
              "      <td>./dataset1/images/Cars131.png</td>\n",
              "      <td>500</td>\n",
              "      <td>300</td>\n",
              "      <td>0.51600</td>\n",
              "      <td>0.456667</td>\n",
              "      <td>0.9560</td>\n",
              "      <td>0.720000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>./dataset1/annotations/Cars293.xml</td>\n",
              "      <td>64</td>\n",
              "      <td>130</td>\n",
              "      <td>160</td>\n",
              "      <td>181</td>\n",
              "      <td>./dataset1/images/Cars293.png</td>\n",
              "      <td>400</td>\n",
              "      <td>267</td>\n",
              "      <td>0.24250</td>\n",
              "      <td>0.638577</td>\n",
              "      <td>0.1650</td>\n",
              "      <td>0.078652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>./dataset1/annotations/Cars222.xml</td>\n",
              "      <td>178</td>\n",
              "      <td>235</td>\n",
              "      <td>158</td>\n",
              "      <td>170</td>\n",
              "      <td>./dataset1/images/Cars222.png</td>\n",
              "      <td>400</td>\n",
              "      <td>230</td>\n",
              "      <td>0.51625</td>\n",
              "      <td>0.713043</td>\n",
              "      <td>0.1425</td>\n",
              "      <td>0.052174</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69567531-7c93-4dde-a97d-505699ee7dd2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-69567531-7c93-4dde-a97d-505699ee7dd2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-69567531-7c93-4dde-a97d-505699ee7dd2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                             filepath  xmin  xmax  ymin  ymax  \\\n",
              "0  ./dataset1/annotations/Cars262.xml   243   285   184   205   \n",
              "1  ./dataset1/annotations/Cars285.xml    91   163   111   134   \n",
              "2  ./dataset1/annotations/Cars131.xml    19   497    29   245   \n",
              "3  ./dataset1/annotations/Cars293.xml    64   130   160   181   \n",
              "4  ./dataset1/annotations/Cars222.xml   178   235   158   170   \n",
              "\n",
              "                        filename  width  height  center_x  center_y  bb_width  \\\n",
              "0  ./dataset1/images/Cars262.png    400     301   0.66000  0.646179    0.1050   \n",
              "1  ./dataset1/images/Cars285.png    400     267   0.31750  0.458801    0.1800   \n",
              "2  ./dataset1/images/Cars131.png    500     300   0.51600  0.456667    0.9560   \n",
              "3  ./dataset1/images/Cars293.png    400     267   0.24250  0.638577    0.1650   \n",
              "4  ./dataset1/images/Cars222.png    400     230   0.51625  0.713043    0.1425   \n",
              "\n",
              "   bb_height  \n",
              "0   0.069767  \n",
              "1   0.086142  \n",
              "2   0.720000  \n",
              "3   0.078652  \n",
              "4   0.052174  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[['filename','width','height']] = df['filepath'].apply(parsing).apply(pd.Series)\n",
        "df.head()\n",
        "\n",
        "# center_x, center_y, width , height\n",
        "df['center_x'] = (df['xmax'] + df['xmin'])/(2*df['width'])\n",
        "df['center_y'] = (df['ymax'] + df['ymin'])/(2*df['height'])\n",
        "\n",
        "df['bb_width'] = (df['xmax'] - df['xmin'])/df['width']\n",
        "df['bb_height'] = (df['ymax'] - df['ymin'])/df['height']\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgqGZKbcuFvY"
      },
      "source": [
        "## 4\\. Dataset Spliting\n",
        "Splitting the dataset of size 433 in 300 images for trainning and 133 for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YO8gdJ1sEuc"
      },
      "outputs": [],
      "source": [
        "### split the data into train and test\n",
        "df_train = df.iloc[:300]\n",
        "df_test = df.iloc[300:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VN1Q7zt5LDjA"
      },
      "outputs": [],
      "source": [
        "train_folder = './yolov5/data_images/train'\n",
        "\n",
        "values = df_train[['filename','center_x','center_y','bb_width','bb_height']].values\n",
        "for fname, x,y, w, h in values:\n",
        "    image_name = os.path.split(fname)[-1]\n",
        "    txt_name = os.path.splitext(image_name)[0]\n",
        "    \n",
        "    dst_image_path = os.path.join(train_folder,image_name)\n",
        "    dst_label_file = os.path.join(train_folder,txt_name+'.txt')\n",
        "    \n",
        "    # copy each image into the folder\n",
        "    copy(fname,dst_image_path)\n",
        "\n",
        "    # generate .txt which has label info\n",
        "    label_txt = f'0 {x} {y} {w} {h}'\n",
        "    with open(dst_label_file,mode='w') as f:\n",
        "        f.write(label_txt)\n",
        "        \n",
        "        f.close()\n",
        "\n",
        "test_folder = './yolov5/data_images/test'\n",
        "\n",
        "values = df_test[['filename','center_x','center_y','bb_width','bb_height']].values\n",
        "for fname, x,y, w, h in values:\n",
        "    image_name = os.path.split(fname)[-1]\n",
        "    txt_name = os.path.splitext(image_name)[0]\n",
        "    \n",
        "    dst_image_path = os.path.join(test_folder,image_name)\n",
        "    dst_label_file = os.path.join(test_folder,txt_name+'.txt')\n",
        "    \n",
        "    # copy each image into the folder\n",
        "    copy(fname,dst_image_path)\n",
        "\n",
        "    # generate .txt which has label info\n",
        "    label_txt = f'0 {x} {y} {w} {h}'\n",
        "    with open(dst_label_file,mode='w') as f:\n",
        "        f.write(label_txt)\n",
        "        \n",
        "        f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNOirpm2tIh7"
      },
      "source": [
        "In order to train for a custom dataset and labeling, it is necessary to build a .yaml file regarding the new labels and the location of the train and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqO_2jCbRPgD"
      },
      "outputs": [],
      "source": [
        "dict_file = {'train' : 'data_images/train' , \n",
        "             'val'   : 'data_images/test',\n",
        "             'nc' : 1,\n",
        "             'names' : ['license_plate'] }\n",
        "\n",
        "with open('./dataset1/data.yaml', 'w') as file:\n",
        "    yaml.dump(dict_file, file, default_flow_style=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6ibnk6rDRqq"
      },
      "source": [
        "## 5\\. Getting the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKfYDkZzBpNt",
        "outputId": "3dcc81f6-dddb-4542-d269-4f0015519935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-10 14:42:46--  https://uporto-my.sharepoint.com/:u:/g/personal/up201806651_up_pt/EcEgEXLvb_RDqed1EwUcge0BRpw_ubtCtMnWHnj1Q5hPGg?download=1\n",
            "Resolving uporto-my.sharepoint.com (uporto-my.sharepoint.com)... 13.107.136.8, 13.107.138.8, 2620:1ec:8f8::8, ...\n",
            "Connecting to uporto-my.sharepoint.com (uporto-my.sharepoint.com)|13.107.136.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /personal/up201806651_up_pt/Documents/5ano/1S/CV/projects/Assig2/report_images.zip?ga=1 [following]\n",
            "--2022-12-10 14:42:47--  https://uporto-my.sharepoint.com/personal/up201806651_up_pt/Documents/5ano/1S/CV/projects/Assig2/report_images.zip?ga=1\n",
            "Reusing existing connection to uporto-my.sharepoint.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 650152 (635K) [application/x-zip-compressed]\n",
            "Saving to: ‘report_images.zip’\n",
            "\n",
            "report_images.zip   100%[===================>] 634.91K  1.58MB/s    in 0.4s    \n",
            "\n",
            "2022-12-10 14:42:48 (1.58 MB/s) - ‘report_images.zip’ saved [650152/650152]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the folder with the images for report\n",
        "\n",
        "if not os.path.exists('report_images'):\n",
        "    !wget --output-document=report_images.zip \"https://uporto-my.sharepoint.com/:u:/g/personal/up201806651_up_pt/EcEgEXLvb_RDqed1EwUcge0BRpw_ubtCtMnWHnj1Q5hPGg?download=1\"\n",
        "    !unzip -q report_images.zip\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElZ21VoI9dL5"
      },
      "source": [
        "### 5.1 Trainning a new model\n",
        "\n",
        "We trained two times with different batch size and 20 iterations.\n",
        "- batch = 8 (that is not divisible by the size of training dataset, 300)\n",
        "- batch = 10 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5Gx5TIShVTQ"
      },
      "outputs": [],
      "source": [
        "def resize_img(image, width):\n",
        "  \n",
        "  img_h, img_w, _ = image.shape\n",
        "  # Resize all images to a specific width (300px)\n",
        "  ratio = img_w / width\n",
        "  dim = (width, (int) (img_h/ratio))\n",
        "  dst = cv2.resize(image, dim, interpolation = cv2.INTER_CUBIC)\n",
        "  return dst\n",
        "\n",
        "# define a function for horizontally \n",
        "# concatenating images of different\n",
        "# heights \n",
        "def hconcat_resize(img_list,interpolation = cv2.INTER_CUBIC):\n",
        "  # take minimum hights\n",
        "  h_min = min(img.shape[0] for img in img_list)\n",
        "    \n",
        "  # image resizing \n",
        "  im_list_resize = [cv2.resize(img,\n",
        "                                (int(img.shape[1] * h_min / img.shape[0]), h_min), \n",
        "                                interpolation = interpolation)\n",
        "                                for img in img_list]\n",
        "    \n",
        "  # return final image\n",
        "  return cv2.hconcat(im_list_resize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For batch size 10\n",
        "# if(train):\n",
        "#   ## train the model\n",
        "#   !python ./yolov5/train.py --data ./dataset1/data.yaml --cfg ./yolov5/models/yolov5s.yaml  --batch-size 10 --name model_name --epochs 20 --cache\n",
        "\n",
        "conf_matrix = cv2.imread(\"./report_images/another_train/confusion_matrix.png\")\n",
        "PRcurve = cv2.imread(\"./report_images/another_train/PR_curve.png\")\n",
        "results = cv2.imread(\"./report_images/another_train/results.png\")\n",
        "\n",
        "# Resize images to 500px width\n",
        "conf_matrix = resize_img(conf_matrix, width=1000)\n",
        "PRcurve = resize_img(PRcurve, width=600)\n",
        "results = resize_img(results, width=800)\n",
        "# # function calling\n",
        "img_h_resize = hconcat_resize([PRcurve, results])\n",
        "  \n",
        "# show the Output image\n",
        "cv2_imshow(conf_matrix)\n",
        "cv2_imshow(img_h_resize)\n",
        "# cv2_imshow(Pcurve)\n",
        "# cv2_imshow(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la5H76h-ce-S"
      },
      "source": [
        "After evaluating the previous trained models, we choose the second one because it has a better performance comparing both with the ideal Precision Recall curve. In other words, for the same precision, the second model have a higger recall, that means that the models classifies less false negatives.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFb9IYIf5478"
      },
      "outputs": [],
      "source": [
        "if(train):\n",
        "  ## train the model\n",
        "  !python ./yolov5/train.py --data ./dataset1/data.yaml --cfg ./yolov5/models/yolov5s.yaml  --batch-size 10 --name model_name --epochs 20 --cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOGCISss6DIr"
      },
      "outputs": [],
      "source": [
        "if(train):  \n",
        "  ## export the model\n",
        "  w_path = \"./yolov5/runs/train/model_name/weights/best.pt\"\n",
        "  !python ./yolov5/export.py --weights w_path --include torchscript onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPu3neGb6LkP"
      },
      "outputs": [],
      "source": [
        "if(train):\n",
        "  ## inference/test model\n",
        "  w_path = \"./yolov5/runs/train/model_name/weights/best.pt\"\n",
        "  !python ./yolov5/detect.py --source ./yolov5/data_images/test/ --weights w_path  \n",
        "  clear_output()\n",
        "  \n",
        "  ## For showing the results\n",
        "  cnt = 0\n",
        "  res = glob('./yolov5/runs/detect/exp/*.png')\n",
        "  for path in res:\n",
        " \n",
        "    if(cnt == 10):\n",
        "      break\n",
        "    cnt += 1\n",
        "    img = cv2.imread(path)\n",
        "    cv2_imshow(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tr8nWgpqyi5w"
      },
      "outputs": [],
      "source": [
        "if train:\n",
        "  ## donwload to disk the model\n",
        "  !zip -r ./trained_model.zip ./yolov5\n",
        "  files.download('./trained_model.zip')\n",
        "  clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvA0cXrB9hB1"
      },
      "source": [
        "### 5.2 Uploading the best trained model trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixxvYmlt6P7M",
        "outputId": "849e8150-a92d-40ff-a0d2-b34bdb0d4b8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-09 17:43:54--  https://uporto-my.sharepoint.com/:u:/g/personal/up201806651_up_pt/EV77qome9ZNBsidKukZn7uwBZXTekAdJOlgBu8unBY1r1Q?download=1\n",
            "Resolving uporto-my.sharepoint.com (uporto-my.sharepoint.com)... 13.107.136.8, 13.107.138.8, 2620:1ec:8f8::8, ...\n",
            "Connecting to uporto-my.sharepoint.com (uporto-my.sharepoint.com)|13.107.136.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /personal/up201806651_up_pt/Documents/5ano/1S/CV/projects/Assig2/best_model.zip?ga=1 [following]\n",
            "--2022-12-09 17:43:55--  https://uporto-my.sharepoint.com/personal/up201806651_up_pt/Documents/5ano/1S/CV/projects/Assig2/best_model.zip?ga=1\n",
            "Reusing existing connection to uporto-my.sharepoint.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30820019 (29M) [application/x-zip-compressed]\n",
            "Saving to: ‘best_model.zip’\n",
            "\n",
            "best_model.zip      100%[===================>]  29.39M  9.53MB/s    in 3.1s    \n",
            "\n",
            "2022-12-09 17:43:58 (9.53 MB/s) - ‘best_model.zip’ saved [30820019/30820019]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## if it is to use a trained model, it necessary to download it    \n",
        "if(train == False):\n",
        "  if not os.path.exists('best_model'):\n",
        "    !wget --output-document=best_model.zip \"https://uporto-my.sharepoint.com/:u:/g/personal/up201806651_up_pt/EV77qome9ZNBsidKukZn7uwBZXTekAdJOlgBu8unBY1r1Q?download=1\"\n",
        "    !unzip -q best_model.zip\n",
        "    !rm best_model.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7uRPiFp9qKW"
      },
      "source": [
        "## 6\\. Some statistics about the testing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRTCXpXSxSuQ"
      },
      "outputs": [],
      "source": [
        "def parsing_XY(path):\n",
        "    parser = xet.parse(path).getroot()\n",
        "    xmin,ymin,xmax,ymax = [],[],[],[]\n",
        "\n",
        "    for parser_XY in parser.iter('object'):\n",
        "      parser_XY = parser_XY.find('bndbox')   \n",
        "      xmin.append(float(parser_XY.find('xmin').text))\n",
        "      ymin.append(float(parser_XY.find('ymin').text))\n",
        "      xmax.append(float(parser_XY.find('xmax').text))\n",
        "      ymax.append(float(parser_XY.find('ymax').text))\n",
        "    \n",
        "    return xmin, ymin, xmax, ymax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JuQ4H8Bqmmp",
        "outputId": "b71c9b14-542f-4cb6-a0fe-108860d2a9ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/hub.py:267: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 🚀 2022-12-9 Python-3.8.16 torch-1.13.0+cu116 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected n license plates vs n of licenses in the gnd truth image\n",
            "   Total images:  133\n",
            "   Matches less:  11\n",
            "   Matches exact:  107\n",
            "   Matches more:  15\n",
            "Detection with more or equal accuracy ( 0.7 ):  99\n"
          ]
        }
      ],
      "source": [
        "found_less_lics = []\n",
        "found_exact_lics = []\n",
        "found_more_lics = []\n",
        "\n",
        "accr = 0.7\n",
        "\n",
        "found_accr = []\n",
        "\n",
        "# Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path='./best_model/weights/best.pt') \n",
        "\n",
        "# Images\n",
        "res = glob('./yolov5/data_images/test/*.png')\n",
        "for path in res:\n",
        "\n",
        "  split =  path.split('/')\n",
        "  img_name = split[len(split)-1]\n",
        "  \n",
        "  file = img_name.split('.')\n",
        "  xml_name = file[0] + \".xml\"\n",
        "  #print(xml_name)\n",
        "  xmin, ymin, xmax, ymax = parsing_XY(\"./dataset1/annotations/\"+ xml_name)\n",
        "  \n",
        "  elem = []\n",
        "  elem.append(xmin)\n",
        "  elem.append(ymin)\n",
        "  elem.append(xmax)\n",
        "  elem.append(ymax)\n",
        "\n",
        "  truth_elem = len(xmin)\n",
        "\n",
        "  results = model(path)\n",
        "  res = results.pandas().xyxy[0]\n",
        "  #results.save()\n",
        "\n",
        "  xmin_est,ymin_est,xmax_est,ymax_est = [],[],[],[]\n",
        "  for idx,row in res.iterrows():\n",
        "    \n",
        "    xm = float(row['xmin'])\n",
        "    ym = float(row['ymin'])\n",
        "    xM = float(row['xmax'])\n",
        "    yM = float(row['ymax'])\n",
        "    xmin_est.append(xm)\n",
        "    ymin_est.append(ym)\n",
        "    xmax_est.append(xM)\n",
        "    ymax_est.append(yM)\n",
        "\n",
        "    if float(row['confidence']) >= accr:\n",
        "      lic_plate = []\n",
        "      lic_plate.append(img_name)\n",
        "      lic_plate.append(xm)\n",
        "      lic_plate.append(ym)\n",
        "      lic_plate.append(xM)\n",
        "      lic_plate.append(yM)\n",
        "\n",
        "      found_accr.append(lic_plate)\n",
        "  \n",
        "  elem_est = []\n",
        "  elem_est.append(xmin_est)\n",
        "  elem_est.append(ymin_est)\n",
        "  elem_est.append(xmax_est)\n",
        "  elem_est.append(ymax_est)\n",
        "\n",
        "  aux = []\n",
        "  aux.append(img_name)\n",
        "  aux.append(elem)\n",
        "  aux.append(elem_est)\n",
        "\n",
        "  detected_elem = len(res)\n",
        "  if detected_elem < truth_elem:\n",
        "    found_less_lics.append(aux)\n",
        "\n",
        "  elif detected_elem == truth_elem:\n",
        "    found_exact_lics.append(aux)\n",
        "\n",
        "  else:\n",
        "    found_more_lics.append(aux)\n",
        "\n",
        "\n",
        "total_img = len(found_less_lics) + len(found_exact_lics) + len(found_more_lics)\n",
        "\n",
        "print(\"Detected n license plates vs n of licenses in the gnd truth image\")\n",
        "print(\"   Total images: \" , total_img)\n",
        "print(\"   Matches less: \" , len(found_less_lics))\n",
        "print(\"   Matches exact: \", len(found_exact_lics))\n",
        "print(\"   Matches more: \" , len(found_more_lics))\n",
        "\n",
        "print(\"Detection with more or equal accuracy (\", accr,\"): \",len(found_accr))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaThwn-oGHRO"
      },
      "source": [
        "The results above are related with the number of license plates detected and it comparision with the label.\n",
        "For instance, if it is not detected any region, it is registered in the \"Less\".\n",
        "The same for the \"More\", that is, if the model predicts more licenses then exists in the labels. In the case of \"Exact\", the model predicts the same number of license plates comparing with the ground truth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcrHPivS9wnM"
      },
      "source": [
        "## 7\\. Visualize some results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1h-uRcjJHfB"
      },
      "outputs": [],
      "source": [
        "def show_results(vec,n=2):\n",
        "  for i in range(n):\n",
        "    pos = random.randint(0,len(vec)-1)\n",
        "\n",
        "    # extract image name\n",
        "    image_name = vec[pos][0]\n",
        "\n",
        "    # extract number of plates in the gnd truth\n",
        "    n_gnd = len(vec[pos][1][0])\n",
        "\n",
        "    # extract number of plates that the model detected\n",
        "    n_est = len(vec[pos][2][0])\n",
        "\n",
        "    img = cv2.imread('./dataset1/images/' + image_name)\n",
        "\n",
        "    print(\"GND: red\")\n",
        "    print(\"EST: blue\")\n",
        "\n",
        "    # draw the gnd truth\n",
        "    for iter in range(n_gnd):\n",
        "      xmin = int(vec[pos][1][0][iter])\n",
        "      ymin = int(vec[pos][1][1][iter])\n",
        "      xmax = int(vec[pos][1][2][iter])\n",
        "      ymax = int(vec[pos][1][3][iter])\n",
        "      start_point = (xmin,ymin)\n",
        "      end_point = (xmax,ymax)\n",
        "      color = (0,0,255)\n",
        "      img = cv2.rectangle(img,start_point,end_point,color,thickness=2)\n",
        "      \n",
        "\n",
        "    # draw the gnd truth\n",
        "    for iter in range(n_est):\n",
        "      xmin = int(vec[pos][2][0][iter])\n",
        "      ymin = int(vec[pos][2][1][iter])\n",
        "      xmax = int(vec[pos][2][2][iter])\n",
        "      ymax = int(vec[pos][2][3][iter])\n",
        "      start_point = (xmin,ymin)\n",
        "      end_point = (xmax,ymax)\n",
        "      color = (255,0,0)\n",
        "      img = cv2.rectangle(img,start_point,end_point,color,thickness=2)\n",
        "    \n",
        "    cv2_imshow(img)\n",
        "    print(image_name)\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_matches_less = 2\n",
        "n_matches_exact = 4\n",
        "n_matches_more = 2\n",
        "\n",
        "n_accurate = 4\n",
        "\n",
        "print(\"Detected less license plates comparing to the truth\")\n",
        "show_results(found_less_lics,n_matches_less)\n",
        "print(\"Detected the exact nº of license plates comparing to the truth\")\n",
        "show_results(found_exact_lics,n_matches_exact)\n",
        "print(\"Detected more license plates comparing to the truth\")\n",
        "show_results(found_more_lics,n_matches_more)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_Zdb687MFWq"
      },
      "source": [
        "## 8\\. Conclusions\n",
        "\n",
        "Some of the labels are misclassified, for instance, some cases have two license plates in the image but the label only have one. This problem can distort the results obtained.\n",
        "\n",
        "Another common problem is the detection of words as license plates because it is more difficult to distinguish, as it can be seen in the above results.\n",
        "\n",
        "There also situtation where the model detect correctly one license plate but the label only has coordinates for one bounding box. So the model is not wrong but incomplete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmXAxaWhJL08"
      },
      "source": [
        "# Part B - License Plate Character Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoUV7okTJU4Z"
      },
      "source": [
        "## 1\\. Dataset and labels loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OfX6ju2JbMM"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import xml.etree.ElementTree as xet\n",
        "\n",
        "from glob import glob\n",
        "from skimage import io\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFUgbk7TJkFD"
      },
      "outputs": [],
      "source": [
        "# downloading datasets\n",
        "if not os.path.exists('dataset2'):\n",
        "    !wget --output-document=dataset2_filtered.zip \"https://uporto-my.sharepoint.com/:u:/g/personal/up201806651_up_pt/EfoDz14pySZGmYNO98zIqdMBH2GDqgxOWiJzoj4MPF1BNA?download=1\"\n",
        "    !unzip -q dataset2_filtered.zip\n",
        "    !rm dataset2_filtered.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zCkb-Be-JkvT",
        "outputId": "5f4959f9-0371-426b-dcab-7cefd40d85ab"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6eb0da2e-4fe0-426c-b45d-8094bae30dbb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filepath</th>\n",
              "      <th>lp_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>./dataset2/annotations/0233.xml</td>\n",
              "      <td>MH01AR5274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>./dataset2/annotations/0013.xml</td>\n",
              "      <td>TS09EB1458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>./dataset2/annotations/0102.xml</td>\n",
              "      <td>DL7SBS6930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>./dataset2/annotations/0215.xml</td>\n",
              "      <td>MH14TCF460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>./dataset2/annotations/0164.xml</td>\n",
              "      <td>HR26CT6702</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6eb0da2e-4fe0-426c-b45d-8094bae30dbb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6eb0da2e-4fe0-426c-b45d-8094bae30dbb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6eb0da2e-4fe0-426c-b45d-8094bae30dbb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                          filepath     lp_text\n",
              "0  ./dataset2/annotations/0233.xml  MH01AR5274\n",
              "1  ./dataset2/annotations/0013.xml  TS09EB1458\n",
              "2  ./dataset2/annotations/0102.xml  DL7SBS6930\n",
              "3  ./dataset2/annotations/0215.xml  MH14TCF460\n",
              "4  ./dataset2/annotations/0164.xml  HR26CT6702"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## label extration from the xml files\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "path = glob('./dataset2/annotations/*.xml')\n",
        "labels_text_dict = dict(filepath=[], lp_text=[] )\n",
        "cnt = 0\n",
        "for filename in path:\n",
        "  # load and parse the file\n",
        "  xmlTree = ET.parse(filename)\n",
        "  chars = []\n",
        "  this_lp_string = ''\n",
        "  for elem in xmlTree.iter():\n",
        "      if( elem.tag == 'object' ):\n",
        "        character = elem.find('name').text\n",
        "        chars.append(character)\n",
        "  for x in chars:\n",
        "    this_lp_string += x\n",
        "    \n",
        "  labels_text_dict['filepath'].append(filename)\n",
        "  labels_text_dict['lp_text'].append(this_lp_string)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(labels_text_dict)\n",
        "df.to_csv('labels_text.csv',index=False)\n",
        "df.head()  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fqiXutKJpBJ"
      },
      "source": [
        "## 2\\. Method 1 - Optical Character Recognition (full image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LPVtjb9Juq0"
      },
      "source": [
        "### 2.1 Pytesseract \n",
        "\n",
        "This was the algorythm that was described in the paper we chose to replicate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0ABBQRSJ4o-"
      },
      "outputs": [],
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "N-4uaqEvJ78-",
        "outputId": "8035dfc1-5430-4f5d-e1a8-6d9e6f0fdfc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "License plates identified properly (all characters): 18 / 178\n",
            "Individual characters correct detections: 504 / 1730\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1c625441-3be3-41a3-9f09-49964bee57c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filepath</th>\n",
              "      <th>lp_text</th>\n",
              "      <th>lp_size</th>\n",
              "      <th>lp_det</th>\n",
              "      <th>n_chars_correct</th>\n",
              "      <th>correct_detection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>./dataset2/images/0233.png</td>\n",
              "      <td>MH01AR5274</td>\n",
              "      <td>10</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>./dataset2/images/0102.png</td>\n",
              "      <td>DL7SBS6930</td>\n",
              "      <td>10</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>./dataset2/images/0164.png</td>\n",
              "      <td>HR26CT6702</td>\n",
              "      <td>10</td>\n",
              "      <td>HR26CT6702</td>\n",
              "      <td>10</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>./dataset2/images/0065.png</td>\n",
              "      <td>TN38M6G162</td>\n",
              "      <td>10</td>\n",
              "      <td>TN38MG6162</td>\n",
              "      <td>8</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>./dataset2/images/0132.png</td>\n",
              "      <td>AP20N3100</td>\n",
              "      <td>9</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>./dataset2/images/0229.png</td>\n",
              "      <td>HR26BP3543</td>\n",
              "      <td>10</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>./dataset2/images/0069.png</td>\n",
              "      <td>MH02BM5048</td>\n",
              "      <td>10</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>./dataset2/images/0221.png</td>\n",
              "      <td>PB08CX2959</td>\n",
              "      <td>10</td>\n",
              "      <td>PBOBCX2959</td>\n",
              "      <td>8</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>./dataset2/images/0022.png</td>\n",
              "      <td>MH03BS7778</td>\n",
              "      <td>10</td>\n",
              "      <td>AMHO3BS7778</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>./dataset2/images/0001.png</td>\n",
              "      <td>MH02CB4545</td>\n",
              "      <td>10</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c625441-3be3-41a3-9f09-49964bee57c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1c625441-3be3-41a3-9f09-49964bee57c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1c625441-3be3-41a3-9f09-49964bee57c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                       filepath     lp_text  lp_size       lp_det  \\\n",
              "0    ./dataset2/images/0233.png  MH01AR5274       10                \n",
              "1    ./dataset2/images/0102.png  DL7SBS6930       10                \n",
              "2    ./dataset2/images/0164.png  HR26CT6702       10   HR26CT6702   \n",
              "3    ./dataset2/images/0065.png  TN38M6G162       10   TN38MG6162   \n",
              "4    ./dataset2/images/0132.png   AP20N3100        9                \n",
              "..                          ...         ...      ...          ...   \n",
              "173  ./dataset2/images/0229.png  HR26BP3543       10                \n",
              "174  ./dataset2/images/0069.png  MH02BM5048       10                \n",
              "175  ./dataset2/images/0221.png  PB08CX2959       10   PBOBCX2959   \n",
              "176  ./dataset2/images/0022.png  MH03BS7778       10  AMHO3BS7778   \n",
              "177  ./dataset2/images/0001.png  MH02CB4545       10                \n",
              "\n",
              "     n_chars_correct  correct_detection  \n",
              "0                  0              False  \n",
              "1                  0              False  \n",
              "2                 10               True  \n",
              "3                  8              False  \n",
              "4                  0              False  \n",
              "..               ...                ...  \n",
              "173                0              False  \n",
              "174                0              False  \n",
              "175                8              False  \n",
              "176                2              False  \n",
              "177                0              False  \n",
              "\n",
              "[178 rows x 6 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pytesseract as pt\n",
        "\n",
        "comparing_results = dict(filepath=[], lp_text=[], lp_size=[], lp_det=[], n_chars_correct=[], correct_detection=[] )\n",
        "correct_guesses=0\n",
        "total_chars=0\n",
        "total_chars_correct=0\n",
        "total_imgs = 0\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "  \n",
        "  filename_image = xet.parse(row['filepath']).getroot().find('filename').text\n",
        "  filename = os.path.join('./dataset2/images',filename_image)\n",
        "\n",
        "  lp_text = row['lp_text']\n",
        "  lp_size = len(lp_text)\n",
        "  n_chars_correct=0\n",
        "  correct_detection = False\n",
        "\n",
        "  img = cv2.imread(filename)\n",
        "  if img is None:\n",
        "    continue\n",
        "  total_imgs+=1\n",
        "\n",
        "  ## Preprocessing described in the paper\n",
        "  filtered = cv2.bilateralFilter(src=img,d=11,sigmaColor=175,sigmaSpace=175)\n",
        "  filtered = cv2.medianBlur(src=filtered, ksize=3)\n",
        "  filtered = cv2.cvtColor(filtered, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "  ## Using pytesseract \n",
        "  characters_detected = pt.image_to_string(filtered)\n",
        "  char_det_w_space = characters_detected.replace(\" \",\"\")\n",
        "  char_det_w_space = char_det_w_space.replace(\"\\n\",\"\")\n",
        "\n",
        "  ## Pytesseract in this version doesnt support character whitelist, so we have to do it by hand\n",
        "  whitelist_set = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
        "  char_det_clean = ''\n",
        "  for c in char_det_w_space:\n",
        "    if c in whitelist_set:\n",
        "      char_det_clean+=c\n",
        "\n",
        "\n",
        "  ## Verify how many characters were properly identified\n",
        "  for i in range(lp_size):\n",
        "    if i > len(char_det_clean)-1:\n",
        "      break\n",
        "    if lp_text[i] == char_det_clean[i]:\n",
        "      n_chars_correct+=1\n",
        "  \n",
        "  ## If all characters were identified properly, than its a fully correct guess of the license plate\n",
        "  if n_chars_correct == lp_size and len(char_det_clean) == lp_size:\n",
        "    correct_detection=True\n",
        "    correct_guesses+=1\n",
        "\n",
        "  total_chars += lp_size\n",
        "  total_chars_correct += n_chars_correct\n",
        "\n",
        "  comparing_results['filepath'].append(filename)\n",
        "  comparing_results['lp_text'].append(lp_text)\n",
        "  comparing_results['lp_size'].append(lp_size)\n",
        "  comparing_results['lp_det'].append(char_det_clean)\n",
        "  comparing_results['n_chars_correct'].append(n_chars_correct)\n",
        "  comparing_results['correct_detection'].append(correct_detection)\n",
        "\n",
        "\n",
        "print('License plates identified properly (all characters): ' + str(correct_guesses) + ' / ' + str(total_imgs) )\n",
        "print('Individual characters correct detections: ' + str(total_chars_correct) + ' / ' + str(total_chars) )\n",
        "\n",
        "\n",
        "df_final = pd.DataFrame(comparing_results)\n",
        "df_final.to_csv('comparing_results_pytesseract.csv',index=False)\n",
        "df_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgJ_HhTQJ-uk"
      },
      "source": [
        "### 2.2 EasyOCR\n",
        "\n",
        "Using the same algorythm we replaced pytesseract with EasyOCR to see if we can improve the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAUACnk3KHIM"
      },
      "outputs": [],
      "source": [
        "pip install easyocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "cIneMUcLKKa5",
        "outputId": "cebf9c78-9be9-4ac9-c736-478d6fc2eff9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n",
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "License plates identified properly (all characters): 17 / 178\n",
            "Individual characters correct detections: 697 / 1730\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-392b113e-2419-492d-b733-2a69af6e6a0e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filepath</th>\n",
              "      <th>lp_text</th>\n",
              "      <th>lp_size</th>\n",
              "      <th>lp_det</th>\n",
              "      <th>n_chars_correct</th>\n",
              "      <th>correct_detection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>./dataset2/images/0233.png</td>\n",
              "      <td>MH01AR5274</td>\n",
              "      <td>10</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>./dataset2/images/0102.png</td>\n",
              "      <td>DL7SBS6930</td>\n",
              "      <td>10</td>\n",
              "      <td>DLZS</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>./dataset2/images/0164.png</td>\n",
              "      <td>HR26CT6702</td>\n",
              "      <td>10</td>\n",
              "      <td>HR26Ct6702</td>\n",
              "      <td>9</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>./dataset2/images/0065.png</td>\n",
              "      <td>TN38M6G162</td>\n",
              "      <td>10</td>\n",
              "      <td>TN38</td>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>./dataset2/images/0132.png</td>\n",
              "      <td>AP20N3100</td>\n",
              "      <td>9</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>./dataset2/images/0229.png</td>\n",
              "      <td>HR26BP3543</td>\n",
              "      <td>10</td>\n",
              "      <td>HR268P3543</td>\n",
              "      <td>9</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>./dataset2/images/0069.png</td>\n",
              "      <td>MH02BM5048</td>\n",
              "      <td>10</td>\n",
              "      <td>MIH02KMSUAS</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>./dataset2/images/0221.png</td>\n",
              "      <td>PB08CX2959</td>\n",
              "      <td>10</td>\n",
              "      <td>Pb08CX2959</td>\n",
              "      <td>9</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>./dataset2/images/0022.png</td>\n",
              "      <td>MH03BS7778</td>\n",
              "      <td>10</td>\n",
              "      <td>MHO3BS7778</td>\n",
              "      <td>9</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>./dataset2/images/0001.png</td>\n",
              "      <td>MH02CB4545</td>\n",
              "      <td>10</td>\n",
              "      <td>02CB4545</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-392b113e-2419-492d-b733-2a69af6e6a0e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-392b113e-2419-492d-b733-2a69af6e6a0e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-392b113e-2419-492d-b733-2a69af6e6a0e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                       filepath     lp_text  lp_size       lp_det  \\\n",
              "0    ./dataset2/images/0233.png  MH01AR5274       10                \n",
              "1    ./dataset2/images/0102.png  DL7SBS6930       10         DLZS   \n",
              "2    ./dataset2/images/0164.png  HR26CT6702       10   HR26Ct6702   \n",
              "3    ./dataset2/images/0065.png  TN38M6G162       10         TN38   \n",
              "4    ./dataset2/images/0132.png   AP20N3100        9                \n",
              "..                          ...         ...      ...          ...   \n",
              "173  ./dataset2/images/0229.png  HR26BP3543       10   HR268P3543   \n",
              "174  ./dataset2/images/0069.png  MH02BM5048       10  MIH02KMSUAS   \n",
              "175  ./dataset2/images/0221.png  PB08CX2959       10   Pb08CX2959   \n",
              "176  ./dataset2/images/0022.png  MH03BS7778       10   MHO3BS7778   \n",
              "177  ./dataset2/images/0001.png  MH02CB4545       10     02CB4545   \n",
              "\n",
              "     n_chars_correct  correct_detection  \n",
              "0                  0              False  \n",
              "1                  3              False  \n",
              "2                  9              False  \n",
              "3                  4              False  \n",
              "4                  0              False  \n",
              "..               ...                ...  \n",
              "173                9              False  \n",
              "174                1              False  \n",
              "175                9              False  \n",
              "176                9              False  \n",
              "177                2              False  \n",
              "\n",
              "[178 rows x 6 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import easyocr\n",
        "\n",
        "comparing_results = dict(filepath=[], lp_text=[], lp_size=[], lp_det=[], n_chars_correct=[], correct_detection=[] )\n",
        "correct_guesses=0\n",
        "total_chars=0\n",
        "total_chars_correct=0\n",
        "total_imgs = 0\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "  \n",
        "  filename_image = xet.parse(row['filepath']).getroot().find('filename').text\n",
        "  filename = os.path.join('./dataset2/images',filename_image)\n",
        "\n",
        "  lp_text = row['lp_text']\n",
        "  lp_size = len(lp_text)\n",
        "  n_chars_correct=0\n",
        "  correct_detection = False\n",
        "\n",
        "  img = cv2.imread(filename)\n",
        "  if img is None:\n",
        "    continue\n",
        "  total_imgs+=1\n",
        "\n",
        "  ## Preprocessing described in the paper\n",
        "  filtered = cv2.bilateralFilter(src=img,d=11,sigmaColor=175,sigmaSpace=175)\n",
        "  filtered = cv2.medianBlur(src=filtered, ksize=3)\n",
        "  filtered = cv2.cvtColor(filtered, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "  reader = easyocr.Reader(['en'], verbose=False)\n",
        "  output = reader.readtext(filtered)\n",
        "  ## Whitelisting characters like we did with pytesseract\n",
        "  if output: \n",
        "    char_det = output[0][1]\n",
        "    char_det_w_space = char_det.replace(\" \",\"\")\n",
        "    whitelist_set = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
        "    char_det_clean = ''\n",
        "    for c in char_det_w_space:\n",
        "      if c in whitelist_set:\n",
        "        char_det_clean+=c\n",
        "  else:\n",
        "    char_det_clean = ' '\n",
        "\n",
        "\n",
        "  ## Verify how many characters were properly identified\n",
        "  for i in range(lp_size):\n",
        "    if i > len(char_det_clean)-1:\n",
        "      break\n",
        "    if lp_text[i] == char_det_clean[i]:\n",
        "      n_chars_correct+=1\n",
        "  \n",
        "  ## If all characters were identified properly, than its a fully correct guess of the license plate\n",
        "  if n_chars_correct == lp_size and len(char_det_clean) == lp_size:\n",
        "    correct_detection=True\n",
        "    correct_guesses+=1\n",
        "\n",
        "  total_chars += lp_size\n",
        "  total_chars_correct += n_chars_correct\n",
        "\n",
        "  comparing_results['filepath'].append(filename)\n",
        "  comparing_results['lp_text'].append(lp_text)\n",
        "  comparing_results['lp_size'].append(lp_size)\n",
        "  comparing_results['lp_det'].append(char_det_clean)\n",
        "  comparing_results['n_chars_correct'].append(n_chars_correct)\n",
        "  comparing_results['correct_detection'].append(correct_detection)\n",
        "\n",
        "\n",
        "print('License plates identified properly (all characters): ' + str(correct_guesses) + ' / ' + str(total_imgs) )\n",
        "print('Individual characters correct detections: ' + str(total_chars_correct) + ' / ' + str(total_chars) )\n",
        "\n",
        "\n",
        "df_final = pd.DataFrame(comparing_results)\n",
        "df_final.to_csv('comparing_results_easyOCR.csv',index=False)\n",
        "df_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP9CcE-0KNZX"
      },
      "source": [
        "### 2.3 Results discussion\n",
        "\n",
        "This method was not accurate for the correct identification of all characters in license plates, identifying properly only 10% of the dataset.\n",
        "\n",
        "However, the EasyOCR has better performance identifying individual characters.\n",
        "With this, we concluded that the individual character recognition from the EasyOCR maybe an asset that we use for our 2nd method, since it properly identified 40% of all individual characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuKHOW1TKT_w"
      },
      "source": [
        "## 3\\. Method 2 - Individual Character Detection using OpenCV\n",
        "In this method, we are going to do pre-processing using OpenCV.\n",
        "\n",
        "The goal is to apply a cascade of operation in order to build a mask with just the license plate character. With this, it is possible to extract a bounding box for each object and it inclination and perform a rotation of the image making it more horizontal.\n",
        "\n",
        "After this, with the rotated image, the previous process is applied again to take a new bounding box so it is possible to use the easyOCR for classify each individual character.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6pNEYLwPwPf"
      },
      "source": [
        "### 3.1 Image pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbW_smSTKYaP"
      },
      "outputs": [],
      "source": [
        "#rotate the image with given theta value\n",
        "def rotate(img, theta):\n",
        "    rows, cols = img.shape[0], img.shape[1]\n",
        "    image_center = (cols/2, rows/2)\n",
        "    \n",
        "    M = cv2.getRotationMatrix2D(image_center,theta,1)\n",
        "\n",
        "    abs_cos = abs(M[0,0])\n",
        "    abs_sin = abs(M[0,1])\n",
        "\n",
        "    bound_w = int(rows * abs_sin + cols * abs_cos)\n",
        "    bound_h = int(rows * abs_cos + cols * abs_sin)\n",
        "\n",
        "    M[0, 2] += bound_w/2 - image_center[0]\n",
        "    M[1, 2] += bound_h/2 - image_center[1]\n",
        "\n",
        "    # rotate orignal image to show transformation\n",
        "    rotated = cv2.warpAffine(img, M, (bound_w, bound_h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
        "    \n",
        "    return rotated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDz83xh9PgMB"
      },
      "outputs": [],
      "source": [
        "def slope(x1, y1, x2, y2):\n",
        "    if x1 == x2:\n",
        "        return 0\n",
        "    slope = (y2-y1)/(x2-x1)\n",
        "    theta = np.rad2deg(np.arctan(slope))\n",
        "    return theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0OdMk4iPjN7"
      },
      "outputs": [],
      "source": [
        "def identify_boundbox(image):\n",
        "  # This will work on the image receive and return the bounding boxes around\n",
        "  # each character of the license plate and its contours\n",
        "\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Apply Gaussian blurring and thresholding \n",
        "  # to reveal the characters on the license plate\n",
        "  blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "  thresh = cv2.adaptiveThreshold(blurred, 255,\n",
        "      cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 45, 15)\n",
        "\n",
        "  # Perform connected components analysis on the thresholded image and\n",
        "  # initialize the mask to hold only the components we are interested in\n",
        "  _, labels = cv2.connectedComponents(thresh)\n",
        "  mask = np.zeros(thresh.shape, dtype=\"uint8\")\n",
        "  mask_filtered = np.zeros(thresh.shape, dtype=\"uint8\")\n",
        "  \n",
        "  # Set lower bound and upper bound criteria for characters\n",
        "  total_pixels = image.shape[0] * image.shape[1]\n",
        "  lower = total_pixels // 220 # heuristic param, can be tuned if necessary\n",
        "  upper = total_pixels // 20  # heuristic param, can be tuned if necessary\n",
        "\n",
        "  # Loop over the unique components\n",
        "  for (i, label) in enumerate(np.unique(labels)):\n",
        "      \n",
        "      # If this is the background label, ignore it\n",
        "      if label == 0:\n",
        "          continue\n",
        "  \n",
        "      # Otherwise, construct the label mask to display only connected component\n",
        "      # for the current label\n",
        "      labelMask = np.zeros(thresh.shape, dtype=\"uint8\")\n",
        "      labelMask[labels == label] = 255\n",
        "      numPixels = cv2.countNonZero(labelMask)\n",
        "      # If the number of pixels in the component is between lower bound and upper bound, \n",
        "      # add it to our mask and save the numPixels for future outlier treatment\n",
        "      if numPixels > lower and numPixels < upper:\n",
        "          mask = cv2.add(mask, labelMask)\n",
        "  # cv2_imshow(mask)\n",
        "  # Find contours and get bounding box for each contour\n",
        "  cnts, _ = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  # cnts = sorted(cnts, key = cv2.contourArea, reverse = False)\n",
        "  boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n",
        "\n",
        "  return (cnts, boundingBoxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiegyINkPlgB"
      },
      "outputs": [],
      "source": [
        "def deskew(path, image, contours):\n",
        "\n",
        "  # Extract image name, that is, the last position in the path\n",
        "  image_name = path.split('/')\n",
        "  image_name = image_name[len(image_name) - 1]\n",
        "\n",
        "  textImg = image.copy()\n",
        "  # save theta value for each bounding box\n",
        "  # in order to rotate the original image\n",
        "  bb_theta = []\n",
        "  \n",
        "  for idx in range(len(cnts)):\n",
        "      \n",
        "      rect = cv2.minAreaRect(cnts[idx])\n",
        "      box = cv2.boxPoints(rect)\n",
        "      box = np.int0(box)\n",
        "      cv2.drawContours(textImg,[box],0,(0,0,255),2)\n",
        "\n",
        "      #we can filter theta as outlier based on other theta values\n",
        "      #this will help in excluding the rare text region with different orientation from ususla value \n",
        "      theta = slope(box[0][0], box[0][1], box[1][0], box[1][1])\n",
        "      if theta < -45:\n",
        "        theta = (theta+90)\n",
        "      bb_theta.append(theta)\n",
        "          \n",
        "  #find the average of all cumulative theta value\n",
        "  orientation = sum(bb_theta) / len(bb_theta)\n",
        " \n",
        "  #print(\"Image orientation in degress: \", orientation)\n",
        "  finalImage = rotate(image, orientation)\n",
        "  cv2.imwrite(\"./bb_original/\"+image_name, textImg)\n",
        "  # cv2_imshow(textImg)\n",
        "  # # print(orientation)\n",
        "  # cv2_imshow(finalImage)\n",
        "  cv2.imwrite(\"./rotated/\"+image_name, finalImage)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3zjtg_mPnQV"
      },
      "outputs": [],
      "source": [
        "!mkdir ./rotated\n",
        "!mkdir ./bb_original"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwLSezGMP4AW"
      },
      "source": [
        "#### 3.1.1 Normalize image size and apply rotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fifKbWNthkIH"
      },
      "outputs": [],
      "source": [
        "def resize_img(image, width):\n",
        "\n",
        "  img_h, img_w, _ = image.shape\n",
        "  # Resize all images to a specific width (300px)\n",
        "  ratio = img_w / width\n",
        "  dim = (width, (int) (img_h/ratio))\n",
        "  dst = cv2.resize(image, dim, interpolation = cv2.INTER_CUBIC)\n",
        "  return dst\n",
        "\n",
        "# define a function for horizontally \n",
        "# concatenating images of different\n",
        "# heights \n",
        "def hconcat_resize(img_list,interpolation = cv2.INTER_CUBIC):\n",
        "  # take minimum hights\n",
        "  h_min = min(img.shape[0] for img in img_list)\n",
        "    \n",
        "  # image resizing \n",
        "  im_list_resize = [cv2.resize(img,\n",
        "                                (int(img.shape[1] * h_min / img.shape[0]), h_min), \n",
        "                                interpolation = interpolation)\n",
        "                                for img in img_list]\n",
        "    \n",
        "  # return final image\n",
        "  return cv2.hconcat(im_list_resize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgIR7QEAPoxB"
      },
      "outputs": [],
      "source": [
        "path = glob('./dataset2/images/*.png')\n",
        "for i in path:\n",
        "\n",
        "  image = cv2.imread(i)\n",
        "  \n",
        "  # Resize all images to a specific width (300px)\n",
        "  image = resize_img(image,width=300)\n",
        "\n",
        "  # Boundig boxes and contours of each character identified\n",
        "  cnts,_ = identify_boundbox(image)\n",
        "  \n",
        "  # Rotate the image to a more horizontal position and save the result\n",
        "  # in the folder ./rotated\n",
        "  # Also save the original image with the bounding boxes that cause the rotation\n",
        "  # in the folder ./bb_original\n",
        "  deskew(i,image,cnts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_for_result = ['0066.png', '0030.png']\n",
        "for i in range(2):\n",
        "  \n",
        "  original = cv2.imread( './dataset2/images/' + example_for_result[i] )\n",
        "  original = resize_img(original, width=300)\n",
        "  original_bb =  cv2.imread( './bb_original/' + example_for_result[i] )\n",
        "  rotated =  cv2.imread('./rotated/' + example_for_result[i])\n",
        "  \n",
        "  print(\"Resized -> Resized with bounding box -> Rotated\")\n",
        "  res_hconcat = hconcat_resize([original, original_bb, rotated])\n",
        "  cv2_imshow(res_hconcat)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VS2CPyrQL0y"
      },
      "source": [
        " ### 3.2 Individual Character extraction and classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSjsGEGDQMzR"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import easyocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZDDx7aCQXxC"
      },
      "outputs": [],
      "source": [
        "# Sort the bounding boxes from left to right, top to bottom\n",
        "# sort by Y first, and then sort by X if Ys are similar\n",
        "def compare(rect1, rect2):\n",
        "  if abs(rect1[1] - rect2[1]) > 10:\n",
        "    return rect1[1] - rect2[1]\n",
        "  else:\n",
        "    return rect1[0] - rect2[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "AX0IWZakQbKE",
        "outputId": "8581cd23-6997-4b01-a829-4985b1379184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "License plates identified properly (all characters): 22 / 178\n",
            "Individual characters correct detections: 683 / 1730\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-db00b5d7-4829-42c8-9406-c36ec161cd10\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filepath</th>\n",
              "      <th>lp_text</th>\n",
              "      <th>lp_size</th>\n",
              "      <th>lp_det</th>\n",
              "      <th>n_chars_correct</th>\n",
              "      <th>correct_detection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>./dataset2/images/0233.png</td>\n",
              "      <td>MH01AR5274</td>\n",
              "      <td>10</td>\n",
              "      <td>NF</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>./dataset2/images/0102.png</td>\n",
              "      <td>DL7SBS6930</td>\n",
              "      <td>10</td>\n",
              "      <td>SIDS6930</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>./dataset2/images/0164.png</td>\n",
              "      <td>HR26CT6702</td>\n",
              "      <td>10</td>\n",
              "      <td>HER26CT6702</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>./dataset2/images/0065.png</td>\n",
              "      <td>TN38M6G162</td>\n",
              "      <td>10</td>\n",
              "      <td>TeN38MG6162</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>./dataset2/images/0132.png</td>\n",
              "      <td>AP20N3100</td>\n",
              "      <td>9</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>./dataset2/images/0229.png</td>\n",
              "      <td>HR26BP3543</td>\n",
              "      <td>10</td>\n",
              "      <td>HR26BP3543</td>\n",
              "      <td>10</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>./dataset2/images/0069.png</td>\n",
              "      <td>MH02BM5048</td>\n",
              "      <td>10</td>\n",
              "      <td>I2BM</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>./dataset2/images/0221.png</td>\n",
              "      <td>PB08CX2959</td>\n",
              "      <td>10</td>\n",
              "      <td>2959PBOE18CX</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>./dataset2/images/0022.png</td>\n",
              "      <td>MH03BS7778</td>\n",
              "      <td>10</td>\n",
              "      <td>MHI3BS7778</td>\n",
              "      <td>9</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>./dataset2/images/0001.png</td>\n",
              "      <td>MH02CB4545</td>\n",
              "      <td>10</td>\n",
              "      <td>MH2CB545</td>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db00b5d7-4829-42c8-9406-c36ec161cd10')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-db00b5d7-4829-42c8-9406-c36ec161cd10 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-db00b5d7-4829-42c8-9406-c36ec161cd10');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                       filepath     lp_text  lp_size        lp_det  \\\n",
              "0    ./dataset2/images/0233.png  MH01AR5274       10            NF   \n",
              "1    ./dataset2/images/0102.png  DL7SBS6930       10      SIDS6930   \n",
              "2    ./dataset2/images/0164.png  HR26CT6702       10   HER26CT6702   \n",
              "3    ./dataset2/images/0065.png  TN38M6G162       10   TeN38MG6162   \n",
              "4    ./dataset2/images/0132.png   AP20N3100        9                 \n",
              "..                          ...         ...      ...           ...   \n",
              "173  ./dataset2/images/0229.png  HR26BP3543       10    HR26BP3543   \n",
              "174  ./dataset2/images/0069.png  MH02BM5048       10          I2BM   \n",
              "175  ./dataset2/images/0221.png  PB08CX2959       10  2959PBOE18CX   \n",
              "176  ./dataset2/images/0022.png  MH03BS7778       10    MHI3BS7778   \n",
              "177  ./dataset2/images/0001.png  MH02CB4545       10      MH2CB545   \n",
              "\n",
              "     n_chars_correct  correct_detection  \n",
              "0                  0              False  \n",
              "1                  1              False  \n",
              "2                  1              False  \n",
              "3                  2              False  \n",
              "4                  0              False  \n",
              "..               ...                ...  \n",
              "173               10               True  \n",
              "174                0              False  \n",
              "175                0              False  \n",
              "176                9              False  \n",
              "177                4              False  \n",
              "\n",
              "[178 rows x 6 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "comparing_results = dict(filepath=[], lp_text=[], lp_size=[], lp_det=[], n_chars_correct=[], correct_detection=[] )\n",
        "correct_guesses=0\n",
        "total_chars=0\n",
        "total_chars_correct=0\n",
        "total_imgs = 0\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "  \n",
        "  filename_image = xet.parse(row['filepath']).getroot().find('filename').text\n",
        "  filename = os.path.join('./dataset2/images',filename_image)\n",
        "\n",
        "  lp_text = row['lp_text']\n",
        "  lp_size = len(lp_text)\n",
        "  n_chars_correct=0\n",
        "  correct_detection = False\n",
        "\n",
        "  image = cv2.imread('./rotated/' + filename_image)\n",
        "  if image is None:\n",
        "    continue\n",
        "  total_imgs+=1\n",
        "  # print(\"Itr: \",total_imgs)\n",
        "  result = image.copy()\n",
        "  # Boundig boxes and contours of each character identified\n",
        "  cnts,boundingBoxes = identify_boundbox(image)\n",
        "  \n",
        "  # Sort BB by top left point position\n",
        "  boundingBoxes = sorted(boundingBoxes, key=functools.cmp_to_key(compare) )\n",
        "  \n",
        "  license_plate = ''\n",
        "  pad = 5\n",
        "  for bb in boundingBoxes:\n",
        "    x,y,w,h = bb\n",
        "    if (w < pad) or (h < pad):\n",
        "      continue\n",
        "    \n",
        "    y_min = y-pad \n",
        "    y_max = y+h+pad\n",
        "    x_min = x-pad\n",
        "    x_max = x+w+pad\n",
        "\n",
        "    if (y_min < 0):\n",
        "      y_min = 0\n",
        "    if (x_min < 0):\n",
        "      x_min = 0\n",
        "    if ( x_max > image.shape[1]):\n",
        "      x_max = image.shape[1]\n",
        "    if ( y_max > image.shape[0]):\n",
        "      y_max = image.shape[0]\n",
        "\n",
        "    color_red = (0, 0, 255)\n",
        "    cv2.rectangle(result, (x_min, y_min), (x_max, y_max), color_red, 2)\n",
        "    \n",
        "    cropped = image[y_min : y_max, x_min: x_max]\n",
        "    # cv2_imshow(cropped)\n",
        "    reader = easyocr.Reader(['en'], verbose=False)\n",
        "    output = reader.readtext(cropped)\n",
        "    if output: \n",
        "      license_plate += output[0][1]\n",
        "  #cv2_imshow(result)\n",
        "  #print(filename_image)\n",
        "  #print(\"Detection\", license_plate) \n",
        "     \n",
        "  ## Whitelisting characters like we did with pytesseract\n",
        "  whitelist_set = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
        "  char_det_clean = ''\n",
        "\n",
        "  if len(license_plate) > 0:\n",
        "    for c in license_plate:\n",
        "      if c in whitelist_set:\n",
        "        char_det_clean+=c\n",
        "  else:\n",
        "    char_det_clean = ' '\n",
        "\n",
        "  ## Verify how many characters were properly identified\n",
        "  for i in range(lp_size):\n",
        "    if i > len(char_det_clean)-1:\n",
        "      break\n",
        "    if lp_text[i] == char_det_clean[i]:\n",
        "      n_chars_correct+=1\n",
        "  \n",
        "  ## If all characters were identified properly, than its a fully correct guess of the license plate\n",
        "  if n_chars_correct == lp_size and len(char_det_clean) == lp_size:\n",
        "    correct_detection=True\n",
        "    correct_guesses+=1\n",
        "\n",
        "  total_chars += lp_size\n",
        "  total_chars_correct += n_chars_correct\n",
        "\n",
        "  comparing_results['filepath'].append(filename)\n",
        "  comparing_results['lp_text'].append(lp_text)\n",
        "  comparing_results['lp_size'].append(lp_size)\n",
        "  comparing_results['lp_det'].append(char_det_clean)\n",
        "  comparing_results['n_chars_correct'].append(n_chars_correct)\n",
        "  comparing_results['correct_detection'].append(correct_detection)\n",
        "\n",
        "\n",
        "print('License plates identified properly (all characters): ' + str(correct_guesses) + ' / ' + str(total_imgs) )\n",
        "print('Individual characters correct detections: ' + str(total_chars_correct) + ' / ' + str(total_chars) )\n",
        "\n",
        "df_final = pd.DataFrame(comparing_results)\n",
        "df_final.to_csv('comparing_results_ours.csv',index=False)\n",
        "df_final\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hYyWSwhq_aW"
      },
      "source": [
        "### 3.3 Application in european license plates\n",
        "The main dataset that we used has many images of license plates in real scenarios, so we decided to test the method with a few optimal images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# downloading datasets\n",
        "if not os.path.exists('european_plates'):\n",
        "    !wget --output-document=european_plates.zip \"https://uporto-my.sharepoint.com/:u:/g/personal/up201806651_up_pt/EdcB6S410iZMrV_g3edXrW8Bz2vFGGTZqCwmt1kIEWBRPQ?download=1\"\n",
        "    !unzip -q european_plates.zip\n",
        "    !rm european_plates.zip\n",
        "\n",
        "path = glob('./european_plates/*.png')\n",
        "\n",
        "for img_name in path:\n",
        "\n",
        "  image = cv2.imread(img_name)\n",
        " \n",
        "  result = image.copy()\n",
        "  # Boundig boxes and contours of each character identified\n",
        "  cnts,boundingBoxes = identify_boundbox(image)\n",
        "  \n",
        "  # Sort BB by top left point position\n",
        "  boundingBoxes = sorted(boundingBoxes, key=functools.cmp_to_key(compare) )\n",
        "  \n",
        "  license_plate = ''\n",
        "  pad = 5\n",
        "  for bb in boundingBoxes:\n",
        "    x,y,w,h = bb\n",
        "    if (w < pad) or (h < pad):\n",
        "      continue\n",
        "    \n",
        "    y_min = y-pad \n",
        "    y_max = y+h+pad\n",
        "    x_min = x-pad\n",
        "    x_max = x+w+pad\n",
        "\n",
        "    if (y_min < 0):\n",
        "      y_min = 0\n",
        "    if (x_min < 0):\n",
        "      x_min = 0\n",
        "    if ( x_max > image.shape[1]):\n",
        "      x_max = image.shape[1]\n",
        "    if ( y_max > image.shape[0]):\n",
        "      y_max = image.shape[0]\n",
        "\n",
        "    color_red = (0, 0, 255)\n",
        "    cv2.rectangle(result, (x_min, y_min), (x_max, y_max), color_red, 2)\n",
        "    \n",
        "    cropped = image[y_min : y_max, x_min: x_max]\n",
        "    # cv2_imshow(cropped)\n",
        "    reader = easyocr.Reader(['en'], verbose=False)\n",
        "    output = reader.readtext(cropped)\n",
        "    if output: \n",
        "      license_plate += output[0][1]\n",
        "  \n",
        "  cv2_imshow(result)\n",
        "  print(\"Result: \", license_plate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3c4D9htL9ko"
      },
      "source": [
        "### 3.4 Results discussion\n",
        "This method identified almost the same ammount of individual characters when comparing to the method in 2.2, but the full license plate identification was slightly better (12%).\n",
        "\n",
        "This was achieved mostly due to the pre-processing applied, because the dataset that we used has license plates that are not aligned and some even with low quality images.\n",
        "\n",
        "Our method also commonly fails if the license plate has more visual elements other than only characters, because it detects the bounding box of things that are not elements.\n",
        "Even with the bounding box being properly identified, easyOCR can still fail at classifying it.\n",
        "\n",
        "With the purpose of analyzing this last problem, we tested the method with the images presented on 3.3.\n",
        "\n",
        "We can conclude that the bounding box identification done through OpenCV pre processing was working fine for clear license plates, but the easyOCR often fails at identifying some characters like 1, 4, 0/O, 8/B, J, etc.\n",
        "\n",
        "Because of this, we suggest that if there was opportunity to improve this method in the future, it would be achieved by replacing the easyOCR by a CNN that would be modelled and trained by us.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bP9CcE-0KNZX"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
